{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315c0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el handle (MLClient)\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Importamos también del paquete de azure.identity\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "#vamos a autenticar directamente con el navegador\n",
    "\n",
    "credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9882656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.browser.InteractiveBrowserCredential object at 0x107dddd50>,\n",
      "         subscription_id=3382d9b3-bbe5-4548-ad6a-029648476060,\n",
      "         resource_group_name=TFM_AI,\n",
      "         workspace_name=aml_tfm)\n"
     ]
    }
   ],
   "source": [
    "# Una vez autenticados, conectamos al Workspace de Azure Machine Learning. Podemos usar el config.json \n",
    "# o incluirlo aquí\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"3382d9b3-bbe5-4548-ad6a-029648476060\",\n",
    "    resource_group_name=\"TFM_AI\",\n",
    "    workspace_name=\"aml_tfm\",\n",
    ")\n",
    "print (ml_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a18790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias para trabajar con dataset\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "web_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
    "\n",
    "credit_data = Data(\n",
    "    name=\"tarjetas_credito_default\",\n",
    "    path=web_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Dataset para predecir impago en tarjetas de crédito\",\n",
    "    tags={\"source_type\": \"web\", \"source\": \"UCI ML Repo\"},\n",
    "    version=\"1.0.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03163d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with name tarjetas_credito_default was registered to workspace, the dataset version is 1.0.0\n"
     ]
    }
   ],
   "source": [
    "credit_data = ml_client.data.create_or_update(credit_data)\n",
    "print(\n",
    "    f\"Dataset with name {credit_data.name} was registered to workspace, the dataset version is {credit_data.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "848e6966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#puedo acceder a este dataset asignandolo a una variable de la siguient forma\n",
    "credit_dataset = ml_client.data.get(\"tarjetas_credito_default\", version='1.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8e76561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando un nuevo compute cluster...\n",
      "AMLCompute con nombre tfm-cluster se ha creado. Instancia tipo: STANDARD_DS2_V2\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "cpu_compute_target = \"tfm-cluster\"\n",
    "\n",
    "try:\n",
    "    # miramos si existe ya un cluster con el mismo nombre\n",
    "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "    print(\n",
    "        f\"Ya existe un cluster con nombre {cpu_compute_target},lo utilizaremos como está creado.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creando un nuevo compute cluster...\")\n",
    "\n",
    "    # Creamos el Azure ML Compute Cluster con los parámetros indicados\n",
    "    cpu_cluster = AmlCompute(\n",
    "        # Nombre del cluster\n",
    "        name=\"tfm-cluster\",\n",
    "        # Azure ML Compute es el servicio On-demand de cómputo\n",
    "        type=\"amlcompute\",\n",
    "        # VM Serie que vamos a utilizar. Si fuese necesario un clúster con GPU deberíamos utilizar otra serie\n",
    "        size=\"STANDARD_DS2_V2\",\n",
    "        # Número de nodos mínimo en ejecución cuando no hay jobs a ejecutar\n",
    "        min_instances=0,\n",
    "        # Número máximo de nodos del auto escalado.\n",
    "        max_instances=4,\n",
    "        # Tiempo que se espera desde que termina el job hasta que se vuelve a desescalar.\n",
    "        idle_time_before_scale_down=180,\n",
    "        # Con este parámetro podemos indicar si queremos utilizar VM de tipo Low Priority o Regular PAYG\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "\n",
    "    # Ahora le pasamos al handler de ML Client el objeto para que lo actualice y cree.\n",
    "    cpu_cluster = ml_client.begin_create_or_update(cpu_cluster)\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute con nombre {cpu_cluster.name} se ha creado. Instancia tipo: {cpu_cluster.size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "743111a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_dependencias = \"./dependencias\"\n",
    "os.makedirs(dir_dependencias, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ab6d25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./dependencias/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dir_dependencias}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scikit-learn=0.24.2\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - xlrd==2.0.1\n",
    "    - mlflow== 1.26.1\n",
    "    - azureml-mlflow==1.42.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7f38b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment con nombre aml-python-tfm se ha registrado. Version: 1.0\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"aml-python-tfm\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Entorno creado para el TFM para trabajar con el dataset Credit Card Defaults pipeline\",\n",
    "    tags={\"scikit-learn\": \"0.24.2\"},\n",
    "    conda_file=os.path.join(dir_dependencias, \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
    "    version=\"1.0\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment con nombre {pipeline_job_env.name} se ha registrado. Version: {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4cb948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_prep_src_dir = \"./componentes/data_prep\"\n",
    "os.makedirs(data_prep_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3c2bc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./componentes/data_prep/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {data_prep_src_dir}/data_prep.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Funcion principal del script\"\"\"\n",
    "\n",
    "    # Argumentos de entrada/Salida\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"Path a los datos de entrada\")\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path a los datos de entrenamiento\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path a los datos de test\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Empezamos el logging \n",
    "    mlflow.start_run()\n",
    "\n",
    "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    print(\"Datos de entrada\", args.data)\n",
    "    # Creamos nuestro dataframe\n",
    "    credit_df = pd.read_excel(args.data, header=1, index_col=0)\n",
    "\n",
    "    mlflow.log_metric(\"num_samples\", credit_df.shape[0])\n",
    "    mlflow.log_metric(\"num_features\", credit_df.shape[1] - 1)\n",
    "\n",
    "    credit_train_df, credit_test_df = train_test_split(\n",
    "        credit_df,\n",
    "        test_size=args.test_train_ratio,\n",
    "    )\n",
    "\n",
    "    # Los path de salida se montan como directorios y debemos añadir un nombre al fichero\n",
    "    credit_train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
    "\n",
    "    credit_test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
    "\n",
    "    # paramos el logging\n",
    "    mlflow.end_run()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bed4671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "data_prep_component = command(\n",
    "    name=\"data_prep_credit_defaults\",\n",
    "    display_name=\"Data preparation para poder entrenar modelos\",\n",
    "    description=\"Lee un fichero .xls como entrada, hace split a train y test\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_folder\"),\n",
    "        \"test_train_ratio\": Input(type=\"number\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # Directorio origen donde está el fichero con los pasos a ejecutar\n",
    "    code=data_prep_src_dir,\n",
    "    command=\"\"\"python data_prep.py \\\n",
    "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
    "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd9998be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_src_dir = \"./components/train\"\n",
    "os.makedirs(train_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f88801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_src_dir}/train.py\n",
    "import argparse\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Seleccionamos el primer fichero. Asumimos que solo hay uno.\n",
    "    Args:\n",
    "        path (str): path al directorio o fichero a elegir\n",
    "    Returns:\n",
    "        str: path completo del fichero indicado\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "\n",
    "# Iniciamos Logging\n",
    "mlflow.start_run()\n",
    "\n",
    "# Habilitamos autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Función mail del script\"\"\"\n",
    "\n",
    "    # Argumentos para In/Out\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path a los datos para train\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path a los datos para test\")\n",
    "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
    "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"nombre del modelo\")\n",
    "    parser.add_argument(\"--model\", type=str, help=\"path al fichero con el modelo\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #  Los path se montan como directorios y debemos añadir un nombre al fichero\n",
    "    train_df = pd.read_csv(select_first_file(args.train_data))\n",
    "\n",
    "    #  Cogemos la columna con la etiqueta / variable target\n",
    "    y_train = train_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convertimos el Dataframe a un array \n",
    "    X_train = train_df.values\n",
    "\n",
    "    #  Los path se montan como directorios y debemos añadir un nombre al fichero\n",
    "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_test = test_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convertimos a array\n",
    "    X_test = test_df.values\n",
    "\n",
    "    print(f\"Entrenamiento con dimensiones: {X_train.shape}\")\n",
    "\n",
    "    clf = GradientBoostingClassifier(\n",
    "        n_estimators=args.n_estimators, learning_rate=args.learning_rate\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Registramos el modelo en el workspace de Azure Machine Learning\n",
    "    print(\"Registramos el modelo en el workspace usando MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=clf,\n",
    "        registered_model_name=args.registered_model_name,\n",
    "        artifact_path=args.registered_model_name,\n",
    "    )\n",
    "\n",
    "    # Guardamos el modelo en un fichero\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=clf,\n",
    "        path=os.path.join(args.model, \"trained_model\"),\n",
    "    )\n",
    "\n",
    "    # Finalizamos el Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1187c0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/train/train.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_src_dir}/train.yml\n",
    "# <Comienzo definicion componente>\n",
    "name: train_credit_defaults_model\n",
    "display_name: Train Credit Defaults Model\n",
    "# version: 1 # Si no especificamos versión, automáticamente aumentará la versión\n",
    "type: command\n",
    "inputs:\n",
    "  train_data: \n",
    "    type: uri_folder\n",
    "  test_data: \n",
    "    type: uri_folder\n",
    "  learning_rate:\n",
    "    type: number     \n",
    "  registered_model_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # para este paso y como demo del trabajo de TFM, utilizaremos uno de los curated environment, en concreto el que\n",
    "  # incluye las librerias de sklearn\n",
    "  azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:21\n",
    "command: >-\n",
    "  python train.py \n",
    "  --train_data ${{inputs.train_data}} \n",
    "  --test_data ${{inputs.test_data}} \n",
    "  --learning_rate ${{inputs.learning_rate}}\n",
    "  --registered_model_name ${{inputs.registered_model_name}} \n",
    "  --model ${{outputs.model}}\n",
    "# </final definicion componente>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f387211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos el Package Component\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "# Cargamos la configuración del componente desde el fichero yaml\n",
    "train_component = load_component(path=os.path.join(train_src_dir, \"train.yml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bead9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading train (0.0 MBs): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3639/3639 [00:00<00:00, 39292.55it/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Componente train_credit_defaults_model Version: 2022-08-15-16-50-29-2935915 se ha registrado\n"
     ]
    }
   ],
   "source": [
    "# Registramos el componente en el Workspace\n",
    "train_component = ml_client.create_or_update(train_component)\n",
    "\n",
    "print(\n",
    "    f\"Componente {train_component.name} Version: {train_component.version} se ha registrado\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "088d3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# el decorator DSL le indica a Azure ML que estamos definiendo un pipeline\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_target,\n",
    "    description=\"End to End data_prep-train pipeline\",\n",
    ")\n",
    "def credit_defaults_pipeline(\n",
    "    pipeline_job_data_input,\n",
    "    pipeline_job_test_train_ratio,\n",
    "    pipeline_job_learning_rate,\n",
    "    pipeline_job_registered_model_name,\n",
    "):\n",
    "    # Utilizamos la funcion data_prep que definimos anteriormente como una llamada en python\n",
    "    data_prep_job = data_prep_component(\n",
    "        data=pipeline_job_data_input,\n",
    "        test_train_ratio=pipeline_job_test_train_ratio,\n",
    "    )\n",
    "\n",
    "    # Utilizamos la función de training como una llamada en python\n",
    "    train_job = train_component(\n",
    "        train_data=data_prep_job.outputs.train_data,  # Utilizamos las salidas del paso anterior\n",
    "        test_data=data_prep_job.outputs.test_data,  # Utilizamos las salidas del paso anterior\n",
    "        learning_rate=pipeline_job_learning_rate,  # Utilizamos los argumentos del pipeline como parametros\n",
    "        registered_model_name=pipeline_job_registered_model_name,\n",
    "    )\n",
    "\n",
    "    # Un pipeline devuelve un diccionario como salida\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
    "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb47dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_name = \"credit_defaults_model\"\n",
    "\n",
    "# Ahora instanciamos nuestro pipeline\n",
    "pipeline = credit_defaults_pipeline(\n",
    "    # pipeline_job_data_input=credit_data,\n",
    "    pipeline_job_data_input=Input(type=\"uri_file\", path=web_path),\n",
    "    pipeline_job_test_train_ratio=0.2,\n",
    "    pipeline_job_learning_rate=0.25,\n",
    "    pipeline_job_registered_model_name=registered_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c1f825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading data_prep (0.0 MBs): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1473/1473 [00:00<00:00, 34080.66it/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "# submit del job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Nombre que le damos al proyecto\n",
    "    experiment_name=\"TFM_validar_metodologia\",\n",
    ")\n",
    "# Abrimos directamente el pipeline para verlo desde el navegador\n",
    "webbrowser.open(pipeline_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fa4e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Utilzamos un nombre para el endpoint. \n",
    "online_endpoint_name = \"credit-endpoint-tfm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "729289c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint credit-endpoint-tfm state: Succeeded\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    ")\n",
    "\n",
    "# creamos un endpoint Online tipo gestionado\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Endpoint Online para TFM AI\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\n",
    "        \"training_dataset\": \"credit_defaults\",\n",
    "        \"model_type\": \"sklearn.GradientBoostingClassifier\",\n",
    "    },\n",
    ")\n",
    "\n",
    "endpoint = ml_client.begin_create_or_update(endpoint)\n",
    "\n",
    "print(f\"Endpoint {endpoint.name} state: {endpoint.provisioning_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96369422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint \"credit-endpoint-tfm\" with provisioning state \"Succeeded\" is retrieved\n"
     ]
    }
   ],
   "source": [
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "print(\n",
    "    f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6890924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos la última versión del modelo\n",
    "latest_model_version = max(\n",
    "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c35a0959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint credit-endpoint-tfm exists\n",
      "Creating/updating online deployment blue "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done (7m 29s)\n"
     ]
    }
   ],
   "source": [
    "# Seleccionamos el modelo con el nombre + versión sacada del paso anterior\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
    "\n",
    "\n",
    "# Creamos el despliegue\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "blue_deployment = ml_client.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "304a605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_dir = \"./deploy\"\n",
    "os.makedirs(deploy_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f25cd07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./deploy/sample-request.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile {deploy_dir}/sample-request.json\n",
    "{\n",
    "  \"input_data\": {\n",
    "    \"columns\": [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\n",
    "    \"index\": [0, 1],\n",
    "    \"data\": [\n",
    "            [20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0],\n",
    "            [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8]\n",
    "        ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "900eb7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, 0]'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lanzamos una request para probar el testing\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    request_file=\"./deploy/sample-request.json\",\n",
    "    deployment_name=\"blue\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055f878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7465f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfmenv)",
   "language": "python",
   "name": "tfmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
